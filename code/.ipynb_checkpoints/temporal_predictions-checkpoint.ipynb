{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook, I would like to predicting power fails in 10 days periods.**\n",
    "===========================================\n",
    "1. Generate a dataset for this task\n",
    "2. Try out and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import re\n",
    "from IPython.display import Image\n",
    "import itertools\n",
    "from random import randint, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up working directory**\n",
    "=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up Working Directory and Data Files\n",
    "drive = 'G'\n",
    "if drive == 'G':\n",
    "    data_dir = '../01.data/'\n",
    "    output_dir = '../05.outputs/'\n",
    "else:\n",
    "    data_dir = '..\\\\01.data\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in events data (sms2.csv)**\n",
    "==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in observed data-processed\n",
    "e = pd.read_csv(data_dir + 'smsV2.csv',parse_dates=['datetime_rcvd'])\n",
    "\n",
    "e = e.sort_values (by=['device_id','datetime_rcvd'])\n",
    "\n",
    "#Lets add a time stamp which truncates to hour\n",
    "e['datetime_rcvd_hr'] = e['datetime_rcvd'].apply (lambda x: datetime (x.year, x.month, x.day, x.hour))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets get the time bounds of events from the dataset**\n",
    "========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events run from :  Wed Oct 19 10:00:00 2016  to :   Thu Mar 23 16:00:00 2017 \n"
     ]
    }
   ],
   "source": [
    "start = e.datetime_rcvd_hr.min()\n",
    "end = e.datetime_rcvd_hr.max()\n",
    "delta = timedelta(hours = 1)\n",
    "print ('Events run from :  %s  to :   %s ' %(start.ctime(),end.ctime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an a blank hourly data frame to capture events at each hour**\n",
    "======================================================================\n",
    "The idea is that I will fill/link this with actual events (dataframe e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt to use a multi-index**\n",
    "==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of date-hour-box pair is 3728\n"
     ]
    }
   ],
   "source": [
    "dates = [start]\n",
    "step = timedelta(hours=1)\n",
    "\n",
    "lst_boxes = list (e.device_id.unique())\n",
    "\n",
    "while start <= end:\n",
    "    start += step\n",
    "    dates.append(start)\n",
    "\n",
    "tuples = list(itertools.product(*[lst_boxes,dates]))\n",
    "\n",
    "print ('Number of date-hour-box pair is %s' %len(dates))\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(tuples, names=['device_id', 'datetime_rcvd_hr'])\n",
    "\n",
    "#Ideal hourly dataframe would look like this: eb should mean events blank\n",
    "eb = pd.DataFrame(index=idx)\n",
    "\n",
    "eb = eb.sort_index()\n",
    "\n",
    "eb = eb.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Merge the two datasets\n",
    "ev = pd.merge(left=eb,right=e, on=['device_id','datetime_rcvd_hr'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean up events to ensure that for each box, time is bounded by time of actual events**\n",
    "=========================================================================================\n",
    "This ensures that we nevr extrapolate (forecast)....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We add the \n",
    "ev2 = pd.DataFrame (columns=ev.columns)\n",
    "\n",
    "for bx in list(ev.device_id.unique()):\n",
    "    \n",
    "    df_bx = ev[ev.device_id==bx]\n",
    "        \n",
    "    last_event = df_bx.datetime_rcvd.max()\n",
    "    ts_upper = datetime (last_event.year, last_event.month, last_event.day, last_event.hour)\n",
    "    \n",
    "    \n",
    "    first_event = df_bx.datetime_rcvd.min()\n",
    "    ts_lower = datetime (first_event.year, first_event.month, first_event.day, first_event.hour)\n",
    "    \n",
    "    mask = (df_bx['datetime_rcvd_hr'] >= ts_lower) & (df_bx['datetime_rcvd_hr'] <= ts_upper)\n",
    "    \n",
    "    df_bx = df_bx.loc[mask]\n",
    "    \n",
    "    ev2 = ev2.append(df_bx)\n",
    "\n",
    "#Replace ev with ev2\n",
    "ev = ev2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets deal with status of the box**\n",
    "======================================\n",
    "1. Alive (1) - when the box gets monitoring messages (at least 2 in 24 hours)\n",
    "\n",
    "2. Dead (0) - When box gets less than 2ã€€messages in 24  hours\n",
    "\n",
    "So box status is correct only up to 12 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add column to indicate whether box is sending ping messages or not\n",
    "#The two messages to indicate box life are: 'pfail_mon' or 'pon_mon'\n",
    "\n",
    "ev['ping_msg'] = 0\n",
    "\n",
    "ev.ix[ev.msg=='pfail_mon',['ping_msg']] =  1\n",
    "\n",
    "ev.ix[ev.msg=='pon_mon',['ping_msg']] =  1\n",
    "\n",
    "ev = ev.sort_values (by = ['datetime_rcvd_hr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we add a box_state column**\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise box_state to 0\n",
    "ev['box_state'] = 0\n",
    "\n",
    "ev = ev.sort_values (by = ['device_id','datetime_rcvd_hr'])\n",
    "\n",
    "start = ev.datetime_rcvd_hr.min()\n",
    "\n",
    "end_all = ev.datetime_rcvd_hr.max()\n",
    "\n",
    "step = timedelta (hours = 24)\n",
    "\n",
    "while start <= end_all:\n",
    "    end = start + step\n",
    "    \n",
    "    mask = (ev['datetime_rcvd_hr'] >= start) & (ev['datetime_rcvd_hr'] <= end)\n",
    "    \n",
    "    check_sum = ev.loc[mask].ping_msg.sum()\n",
    "    \n",
    "    \n",
    "    if check_sum >= 2:\n",
    "        ev.ix[mask,'box_state'] = 1\n",
    "    else:\n",
    "        ev.ix[mask,'box_state'] = 0\n",
    "    \n",
    "   \n",
    "    start += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets deal with power-state**\n",
    "===================================\n",
    "1. 1-power on\n",
    "2. 0-power off\n",
    "3. -1-power status unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise power-state to -1\n",
    "ev['pwr_state'] = np.nan\n",
    "\n",
    "#To indicate source of pwr_state information\n",
    "ev['pwr_state_src'] = ''\n",
    "\n",
    "#When message is pon_monitoring or power back, set power status to 1\n",
    "on_mask = ev['msg'].isin(['pback','pon_mon'])\n",
    "\n",
    "ev.ix[on_mask,'pwr_state'] = 1\n",
    "\n",
    "ev.ix[on_mask,'pwr_state_src'] = 'actual'\n",
    "\n",
    "off_mask = ev['msg'].isin(['pfail','pfail_mon'])\n",
    "\n",
    "ev.ix[off_mask,'pwr_state'] = 0\n",
    "\n",
    "ev.ix[off_mask,'pwr_state_src'] = 'actual'\n",
    "\n",
    "#Set source of message as actual or interpolated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Just interpolating events to make sure the matrix isnt sparse\n",
    "def interpolate_pwr_state (df,t0,t1,e0,max_intv):\n",
    "        #Deduce power state from message\n",
    "        #Lets check duration of this period\n",
    "        diff_hrs = ((t1-t0).total_seconds())/3600\n",
    "        \n",
    "        #print (t1.ctime(),'--',t1.ctime())\n",
    "        #print ('Duration between events is %s hours'%diff_hrs)\n",
    "        mask = (df['datetime_rcvd_hr'] >= t0) & (df['datetime_rcvd_hr'] <= t1)\n",
    "        ping_msgs = df.loc[mask].ping_msg.sum()\n",
    "        #print (ping_msgs)\n",
    "        \n",
    "        #if duration is <=24 hours and number of monitoring messages is >= 2 interpolate pwr_state\n",
    "        \n",
    "        if diff_hrs <= max_intv:\n",
    "            #scenario one poff\n",
    "            if ping_msgs>=diff_hrs/12:\n",
    "                if e0 == 'pfail' or e0 == 'pfail_mon':\n",
    "                    return 0\n",
    "                elif e0 == 'pon_mon' or e0 == 'pback':\n",
    "                    return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Just interpolating events to make sure the matrix isnt sparse\n",
    "def interpolate_events (df, power_state, max_hrs):\n",
    "    \n",
    "    df = df.sort_values (by =['datetime_rcvd_hr'])\n",
    "    \n",
    "    \n",
    "    events = df[['datetime_rcvd_hr','msg']]\n",
    "    \n",
    "    #extract only events-hopefully they are still in sorted order\n",
    "    events = events.dropna(axis=0, how='any')\n",
    "    \n",
    "    #index of events\n",
    "    idx = events.index\n",
    " \n",
    "    \n",
    "    #a counter \n",
    "    i = 0\n",
    "    \n",
    "    #Loop through all events (2 at a time) and extract corresponding rows in df\n",
    "    while i < len(idx)-1:\n",
    " \n",
    "        t0 = events.ix[idx[i],'datetime_rcvd_hr']\n",
    "      \n",
    "        t1 = events.ix[idx[i+1],'datetime_rcvd_hr']\n",
    "        \n",
    "        diff_hrs = ((t1-t0).total_seconds())/3600\n",
    "        \n",
    "        #Event at start hour\n",
    "        ev0 = events.ix[idx[i],'msg']\n",
    "        \n",
    "        #Event at end hour\n",
    "        ev1 = events.ix[idx[i+1],'msg']\n",
    "        \n",
    "        #Rows in df matching events in this interval\n",
    "        mask = (df['datetime_rcvd_hr'] > t0) & (df['datetime_rcvd_hr'] < t1)\n",
    "        #print (len(mask[mask==True]))\n",
    "        \n",
    "\n",
    "        #Set all events in the range to ev0\n",
    "        df.ix[mask,'msg'] = ev0\n",
    "        \n",
    "        #Set source as 'interpolated'\n",
    "        df.ix[mask,'msg_src'] = 'interpolated'\n",
    "        \n",
    "        #Also interpolate power state\n",
    "        if power_state:\n",
    "            #increment counter\n",
    "            pwr = interpolate_pwr_state (df,t0,t1,ev0,max_hrs)\n",
    "            #set power-state to return value\n",
    "            #Set all events in the range to ev0\n",
    "            df.ix[mask,'pwr_state'] = pwr\n",
    "        \n",
    "            #Set source as 'interpolated'\n",
    "            df.ix[mask,'pwr_state_src'] = 'interpolated'\n",
    "            \n",
    "        i +=1\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now interpolate events and label them as such**\n",
    "=================================================\n",
    "The method being used is simple:\n",
    "The time slots x such that x > a and x < b \n",
    "between two events (a,b) takes the event\n",
    "a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on box 0 of 300\n",
      "Working on box 25 of 300\n",
      "Working on box 50 of 300\n",
      "Working on box 75 of 300\n",
      "Working on box 100 of 300\n",
      "Working on box 125 of 300\n",
      "Working on box 150 of 300\n",
      "Working on box 175 of 300\n",
      "Working on box 200 of 300\n",
      "Working on box 225 of 300\n",
      "Working on box 250 of 300\n",
      "Working on box 275 of 300\n"
     ]
    }
   ],
   "source": [
    "#Label actual events as 'actual\n",
    "#and rest of the events as interpolated\n",
    "actual_msgs = ev.loc[ev.msg.notnull()].index\n",
    "\n",
    "ev.ix[actual_msgs, 'msg_src'] = 'actual'\n",
    "\n",
    "boxes = list (ev.device_id.unique())\n",
    "\n",
    "#Load new data into this empty dataframe\n",
    "evi = pd.DataFrame (columns=ev.columns)\n",
    "\n",
    "for i, b in enumerate(boxes):\n",
    "    if i%25 == 0:\n",
    "        print ('Working on box %s of %s'%(i,len(boxes)))\n",
    "    \n",
    "    df_bx = ev[ev.device_id==b]\n",
    "    \n",
    "    evi = evi.append(interpolate_events (df_bx,True, 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add box details\n",
    "bx = pd.read_csv(data_dir + 'Boxes.csv',parse_dates=['DateCollectionStart'])\n",
    "\n",
    "bx = bx[['ClusterId','DateCollectionStart','LONG','LAT', 'BoxID']]\n",
    "\n",
    "bx.rename(columns={'ClusterId': 'psu','LONG':'lon','LAT':'lat','BoxID':'device_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge with evi:\n",
    "evi = evi.sort_values(by = ['device_id','datetime_rcvd_hr'])\n",
    "\n",
    "evi2 = pd.merge (left = evi, right = bx, on = 'device_id', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine whether one box can predict the other in one cluster**\n",
    "===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A utility function to compare \n",
    "def compare (row,col):\n",
    "    if row[col + '_bx1'] == row[col + '_bx2']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_events ():\n",
    "    i = randint(0,3)\n",
    "    ev = ['pfail_mon', 'pfail', 'pback', 'pon_mon']\n",
    "    ev = random.sample(ev, len(ev)) \n",
    "    \n",
    "    return ev[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given two boxes in the cluster(psu), this function simply\n",
    "#join the data of the two boxes based on same time\n",
    "#and compare events on those times\n",
    "def evaluate_nearest_box_predictor(bx1, bx2):\n",
    "    #keep only actual events\n",
    "    bx1 = bx1[bx1.msg_src=='actual']\n",
    "    bx2 = bx2[bx2.msg_src=='actual']\n",
    "        \n",
    "    #Lets count equal events (messages)\n",
    "    use_cols = ['device_id','msg','datetime_rcvd_hr','datetime_rcvd']\n",
    "    bx1 = bx1[use_cols]\n",
    "    bx2 = bx2[use_cols]\n",
    "        \n",
    "    #Merge-keep only events which are common\n",
    "    bx12 = pd.merge(left=bx1,right=bx2, on = 'datetime_rcvd_hr', how='inner', suffixes=['_bx1','_bx2'])\n",
    "        \n",
    "    bx12['compare_msgs'] = bx12.apply(lambda x: compare(x,'msg'), axis=1)\n",
    "        \n",
    "    #Return proportion of equal events as accuracy\n",
    "    val_cnts = bx12.compare_msgs.value_counts()\n",
    "\n",
    "    acc = (val_cnts[1]/bx12.shape[0])*100\n",
    "        \n",
    "    \n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#join the data of the two boxes based on same time\n",
    "#and compare events on those times\n",
    "def evaluate_nearest_box_predictor2(bx1, bx2, rand):\n",
    "    #keep only actual events\n",
    "    bx1 = bx1[bx1.msg_src=='actual']\n",
    "    bx2 = bx2[bx2.msg_src=='actual']\n",
    "    \n",
    "    if rand:\n",
    "        #Lets count equal events (messages)\n",
    "        use_cols = ['device_id','msg','datetime_rcvd_hr','datetime_rcvd']\n",
    "        bx1 = bx1[use_cols]\n",
    "        bx2 = bx2[use_cols]\n",
    "        \n",
    "        #Replace bx2 with random messages\n",
    "        #+++++++++++++++++++++++++++++++++++++++ \n",
    "        bx2['msg'] = bx2.apply(lambda x: random_events(),axis=1)\n",
    "       \n",
    "        #Merge-keep only events which are common\n",
    "        bx12 = pd.merge(left=bx1,right=bx2, on = 'datetime_rcvd_hr', how='inner', suffixes=['_bx1','_bx2'])\n",
    "\n",
    "        bx12['compare_msgs'] = bx12.apply(lambda x: compare(x,'msg'), axis=1)\n",
    "        \n",
    "        #Return proportion of equal events as accuracy\n",
    "        val_cnts = bx12.compare_msgs.value_counts()\n",
    "\n",
    "        acc_1 = (val_cnts[1]/bx12.shape[0])*100\n",
    "        \n",
    "        #+++++++++++++++++++++++++++++++++++++++ \n",
    "        bx1['msg'] = bx1.apply(lambda x: random_events(),axis=1)\n",
    "\n",
    "        #Merge-keep only events which are common\n",
    "        bx12 = pd.merge(left=bx1,right=bx2, on = 'datetime_rcvd_hr', how='inner', suffixes=['_bx1','_bx2'])\n",
    "\n",
    "        bx12['compare_msgs'] = bx12.apply(lambda x: compare(x,'msg'), axis=1)\n",
    "\n",
    "        #Return proportion of equal events as accuracy\n",
    "        val_cnts = bx12.compare_msgs.value_counts()\n",
    "\n",
    "        acc_2 = (val_cnts[1]/bx12.shape[0])*100\n",
    "        \n",
    "        return np.max([acc_2, acc_1])\n",
    "    \n",
    "    else:\n",
    "        #Lets count equal events (messages)\n",
    "        use_cols = ['device_id','msg','datetime_rcvd_hr','datetime_rcvd']\n",
    "        bx1 = bx1[use_cols]\n",
    "        bx2 = bx2[use_cols]\n",
    "\n",
    "        #Merge-keep only events which are common\n",
    "        bx12 = pd.merge(left=bx1,right=bx2, on = 'datetime_rcvd_hr', how='inner', suffixes=['_bx1','_bx2'])\n",
    "\n",
    "        bx12['compare_msgs'] = bx12.apply(lambda x: compare(x,'msg'), axis=1)\n",
    "\n",
    "        #Return proportion of equal events as accuracy\n",
    "        val_cnts = bx12.compare_msgs.value_counts()\n",
    "\n",
    "        acc = (val_cnts[1]/bx12.shape[0])*100\n",
    "\n",
    "\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now consider each cluster and compute accuracy (proportion of events which match)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Clusters Evaluated: 137\n",
      "Accuracy (%)-summary across all boxes : \n",
      "Worst case: 3.125  | Best case : 100.0  | Median : 57.4854558127  | Std deviation : 15.9750785311\n"
     ]
    }
   ],
   "source": [
    "lst_psu = list(evi2.psu.unique())\n",
    "\n",
    "tot = len(lst_psu)\n",
    "acc_all = []\n",
    "for p in lst_psu:\n",
    "    try:\n",
    "        evi_p = evi2[evi2.psu==p]\n",
    "    \n",
    "        boxes = list(evi_p.device_id.unique())\n",
    "    \n",
    "        if len(boxes)>2:\n",
    "            #print ('Boxes: %s'%len(boxes))\n",
    "            tot -= 1\n",
    "            continue\n",
    "        \n",
    "        bx1,bx2 = evi_p[evi_p.device_id==boxes[0]], evi_p[evi_p.device_id==boxes[1]]\n",
    "    \n",
    "        acc = evaluate_nearest_box_predictor2(bx1,bx2,False)\n",
    "        #print (acc)\n",
    "        acc_all.append(acc)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        pass\n",
    "    \n",
    "    #print ('PSU-%s, accuracy: %s'%(p,acc))\n",
    "\n",
    "print ('Total Clusters Evaluated: %s'%tot)\n",
    "print ('Accuracy (%)-summary across all boxes : ')\n",
    "print ('Worst case: %s'%(np.min(acc_all)),\n",
    "       ' | Best case : %s'%(np.max(acc_all)),\n",
    "       ' | Median : %s'%(np.median(acc_all)),' | Std deviation : %s'%(np.std(acc_all)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets see how a random predictor would do...........**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Clusters Evaluated: 137\n",
      "Accuracy (%)-summary across all boxes : \n",
      "Worst case: 12.5  | Best case : 100.0  | Median : 26.3157894737  | Std deviation : 9.48622241919\n"
     ]
    }
   ],
   "source": [
    "lst_psu = list(evi2.psu.unique())\n",
    "\n",
    "tot = len(lst_psu)\n",
    "acc_all = []\n",
    "for p in lst_psu:\n",
    "    try:\n",
    "        evi_p = evi2[evi2.psu==p]\n",
    "    \n",
    "        boxes = list(evi_p.device_id.unique())\n",
    "    \n",
    "        if len(boxes)>2:\n",
    "            #print ('Boxes: %s'%len(boxes))\n",
    "            tot -= 1\n",
    "            continue\n",
    "        \n",
    "        bx1,bx2 = evi_p[evi_p.device_id==boxes[0]], evi_p[evi_p.device_id==boxes[1]]\n",
    "    \n",
    "        acc = evaluate_nearest_box_predictor2(bx1,bx2,True)\n",
    "        #print (acc)\n",
    "        acc_all.append(acc)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    #print ('PSU-%s, accuracy: %s'%(p,acc))\n",
    "\n",
    "print ('Total Clusters Evaluated: %s'%tot)\n",
    "print ('Accuracy (%)-summary across all boxes : ')\n",
    "print ('Worst case: %s'%(np.min(acc_all)),\n",
    "       ' | Best case : %s'%(np.max(acc_all)),\n",
    "       ' | Median : %s'%(np.median(acc_all)),' | Std deviation : %s'%(np.std(acc_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So,  using events from another box seem to do better than random guessing of events**\n",
    "========================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_event_freq (df, t0, t1, by_box, cond):\n",
    "    \"\"\"\n",
    "    This function returns a key value pair where the key is hour of the day\n",
    "    and value is another key-value pair of event and frequencies\n",
    "\n",
    "    @param df: reasonably preprocessed data\n",
    "    @param boxes: boxes to use to compute the freqs\n",
    "    @param t0,t1: Time interval to compute the events in\n",
    "    @param cond: e.g, consider weekday events only (To be implemented later)\n",
    "    @return: Returns a key value pair where the key is hour of the day\n",
    "    and value is another key-value pair of event and frequencies\n",
    "    @raise keyError: raises an exception\n",
    "    \"\"\"\n",
    "    mask = (df['datetime_rcvd_hr'] > t0) & (df['datetime_rcvd_hr'] < t1)\n",
    "    \n",
    "    #Events\n",
    "    df = df.loc[mask]\n",
    "     \n",
    "    if by_box:\n",
    "        #Add hour column\n",
    "        df['hr'] = df['datetime_rcvd_hr'].apply(lambda x: x.hour)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        df['hr'] = df['datetime_rcvd_hr'].apply(lambda x: x.hour)\n",
    "        \n",
    "        hr_cnts = df.groupby(['hr','msg'])['msg'].agg(['count'])\n",
    "       \n",
    "        hr_cnts = hr_cnts.reset_index()\n",
    "            \n",
    "    \n",
    "    return hr_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_time_window_predictor(train_data, pred_date, window_length):\n",
    "    t1 = pred_date - timedelta(days=window_length)\n",
    "    \n",
    "    t0 = pred_date\n",
    "   \n",
    "    t2 = pred_date + timedelta(days=window_length)\n",
    "   \n",
    "    freqs = get_event_freq (train_data, t1, t2, False, False)\n",
    "    \n",
    "    pred_hr = pred_date.hour\n",
    "    \n",
    "    events_hr = freqs[freqs.hr==pred_hr]\n",
    "    \n",
    "    return events_hr.max(axis=0).msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets evaluate how a simple window based predictor would do....**\n",
    "This works loosely on the idea of periodicity in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_temporal_window_predictor(bx, test_id, prop_test, window):\n",
    "    test_bx = bx[bx.device_id==test_id]\n",
    "    \n",
    "    event_dates = list(test_bx.datetime_rcvd_hr)\n",
    "\n",
    "    num_tests = int (test_bx.shape[0]*prop_test)\n",
    "    \n",
    "        \n",
    "    correct = 0\n",
    "    tot = 0\n",
    "    for i in range(num_tests):\n",
    "        test_date = event_dates[randint(0,num_tests)]\n",
    "\n",
    "        \n",
    "        #They are multiple events in some cases-only pick first one\n",
    "        actual_events = test_bx[test_bx.datetime_rcvd_hr==test_date].msg.values\n",
    "        \n",
    "        actual = actual_events[0]\n",
    "        \n",
    "        #Remove this row from trainign data\n",
    "        train = bx[bx.datetime_rcvd_hr!=test_date]\n",
    "        #print (bx.shape[0], '|', train.shape[0],'|', len(actual_events))   \n",
    "        \n",
    "        #predict\n",
    "        predicted = moving_time_window_predictor(train,test_date,window)\n",
    "            \n",
    "        if predicted == actual:\n",
    "            correct += 1\n",
    "            \n",
    "        tot +=1\n",
    "        \n",
    "    return correct/tot*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Clusters Evaluated: 143\n",
      "Accuracy (%)-summary across all boxes : \n",
      "Worst case: 31.875  | Best case : 88.8888888889  | Median : 53.4792368126  | Std deviation : 18.7375790729\n"
     ]
    }
   ],
   "source": [
    "#use only actual events for this\n",
    "evi2_a = evi2[evi2.msg_src=='actual']\n",
    "\n",
    "lst_psu = list(evi2_a.psu.unique())\n",
    "\n",
    "tot = len(lst_psu)\n",
    "acc_all = []\n",
    "for p in lst_psu[:5]:\n",
    "    try:\n",
    "        evi_p = evi2_a[evi2_a.psu==p]\n",
    "        \n",
    "        boxes = list(evi_p.device_id.unique())\n",
    "    \n",
    "        if len(boxes)>2:\n",
    "            #print ('Boxes: %s'%len(boxes))\n",
    "            tot -= 1\n",
    "            continue\n",
    "        \n",
    "        acc1 = evaluate_temporal_window_predictor(evi_p, boxes[0] , 0.25, 7)\n",
    "        \n",
    "        acc2 = evaluate_temporal_window_predictor(evi_p, boxes[1] , 0.25, 7)\n",
    "        \n",
    "        acc_all = acc_all + [acc1,acc2]\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "#print ('PSU-%s, accuracy: %s'%(p,acc))\n",
    "\n",
    "print ('Total Clusters Evaluated: %s'%tot)\n",
    "print ('Accuracy (%)-summary across all boxes : ')\n",
    "print ('Worst case: %s'%(np.min(acc_all)),\n",
    "       ' | Best case : %s'%(np.max(acc_all)),\n",
    "       ' | Median : %s'%(np.median(acc_all)),' | Std deviation : %s'%(np.std(acc_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
